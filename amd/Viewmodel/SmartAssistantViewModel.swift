//  SmartAssistantViewModel.swift
//  amd
//
//  Created by reema aljohani on 12/3/25.
//

import Foundation
import AVFoundation
import Speech
import Combine

class SmartAssistantViewModel: NSObject, ObservableObject {
    
    // MARK: - Published States (UI Binding)
    @Published var isRecording: Bool = false
    @Published var isProcessing: Bool = false
    @Published var isAIProcessing: Bool = false
    
    @Published var realTimeText: String = ""
    @Published var finalText: String = ""
    @Published var simplifiedText: String = ""
    @Published var aiError: String? = nil
    
    
    // MARK: - Speech Engine
    private let audioEngine = AVAudioEngine()
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ar-SA"))
    private var request = SFSpeechAudioBufferRecognitionRequest()
    private var recognitionTask: SFSpeechRecognitionTask?
    

    // MARK: - Init
    override init() {
        super.init()
        requestSpeechPermission()
    }
    
    
    // MARK: - Permissions
    func requestSpeechPermission() {
        SFSpeechRecognizer.requestAuthorization { status in
            if status != .authorized {
                print("Speech permission not granted")
            }
        }
    }
    
    
    // MARK: - Start Recording
    func startRecording() {
        
        guard !isAIProcessing else {
            print("AI still processing â€” cannot start new recording.")
            return
        }
        
        cleanup()
        
        isRecording = true
        isProcessing = false
        
        realTimeText = ""
        finalText = ""
        simplifiedText = ""
        
        request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = true
        
        setupAudioSession()
        startAudioEngine()
        startRecognitionTask()
        
        print("ğŸ™ï¸ Recording started.")
    }
    
    
    // MARK: - Stop Recording
    func stopRecording() {
        
        guard isRecording else { return }
        
        print("Stopping recording...")
        
        isRecording = false
        isProcessing = true
        
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionTask?.finish()
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.6) {
            
            self.isProcessing = false
            
            guard !self.finalText.trimmingCharacters(in: .whitespaces).isEmpty else {
                print("No speech detected.")
                return
            }
            
            self.simplifyText()
        }
    }
    
    
    // MARK: - Audio Session
    private func setupAudioSession() {
        let session = AVAudioSession.sharedInstance()
        
        do {
            try session.setCategory(.record, mode: .measurement, options: .duckOthers)
            try session.setActive(true)
        } catch {
            print("Audio session error:", error.localizedDescription)
        }
    }
    
    
    // MARK: - Start Audio Engine
    private func startAudioEngine() {
        let inputNode = audioEngine.inputNode
        let format = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
            self.request.append(buffer)
        }
        
        do {
            try audioEngine.start()
        } catch {
            print("Audio engine failed:", error.localizedDescription)
        }
    }
    
    
    // MARK: - Recognition Task
    private func startRecognitionTask() {
        
        recognitionTask = speechRecognizer?.recognitionTask(with: request) { result, error in
            
            if let result = result {
                DispatchQueue.main.async {
                    self.realTimeText = result.bestTranscription.formattedString
                    
                    if result.isFinal {
                        self.finalText = self.realTimeText
                        self.realTimeText = ""
                        self.isProcessing = false
                    }
                }
            }
            
            if error != nil {
                DispatchQueue.main.async {
                    print("Recognition error:", error!.localizedDescription)
                    self.isRecording = false
                    self.isProcessing = false
                }
            }
        }
    }
    
    // MARK: - Cleanup (Prevents Bugs)
    private func cleanup() {
        recognitionTask?.cancel()
        recognitionTask = nil
        audioEngine.reset()
    }
    
    
    // MARK: - AI: Simplify Text
    func simplifyText() {
        
        guard !isAIProcessing else { return }
        guard !finalText.isEmpty else { return }
        
        isAIProcessing = true
        simplifiedText = ""
        aiError = nil
        
        print("Sending text to AIâ€¦")
        
        // NEW: safer log (instead of printing the actual key)
        print("API Key Loaded:", !APIKeyManager.shared.openAIKey.isEmpty)

        let url = URL(string: "https://api.openai.com/v1/chat/completions")!
        
        let headers = [
            "Content-Type": "application/json",
            "Authorization": "Bearer \(APIKeyManager.shared.openAIKey)"
        ]
        
        
        let prompt = """
        Ø£Ø±ÙŠØ¯Ùƒ Ø£Ù† ØªØ¹ÙŠØ¯ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¨Ø³Ù‘Ø·Ø© Ø¬Ø¯Ù‹Ø§:

        - ÙƒÙ„Ù…Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.
        - Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø©.
        - Ø¨Ø¯ÙˆÙ† ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©.
        - Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ØªØ¹Ù‚ÙŠØ¯ Ù„ØºÙˆÙŠ.
        - Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø´Ø®Øµ Ù„Ø¯ÙŠÙ‡ Ø¶Ø¹Ù ÙÙŠ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø£Ùˆ Ø´Ø®Øµ Ø£ØµÙ… ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø¨Ø³Ø·Ø©.
        - Ø£Ø¹Ø·Ù Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø²ÙŠØ§Ø¯Ø© Ø£Ùˆ Ø´Ø±Ø­ Ø·ÙˆÙŠÙ„.

        Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:
        \(finalText)

        Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ† Ù…Ø¨Ø³Ù‘Ø·ØªÙŠÙ† ÙÙ‚Ø·.
        """

        
        let body: [String: Any] = [
            "model": "gpt-4o-mini",
            "messages": [
                ["role": "user", "content": prompt]
            ]
        ]
        
        let jsonData = try! JSONSerialization.data(withJSONObject: body)
        
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.allHTTPHeaderFields = headers
        request.httpBody = jsonData
        
        print("SENDING TO AI:")
        print("Final text:", finalText)
        
        print("API Key exists:", !APIKeyManager.shared.openAIKey.isEmpty)

        
        URLSession.shared.dataTask(with: request) { data, response, error in
            
            if let error = error {
                DispatchQueue.main.async {
                    self.aiError = "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø§Ø¯Ù…."
                    self.isAIProcessing = false
                }
                print("AI Error:", error.localizedDescription)
                return
            }
            
            guard let data = data else { return }
            
            do {
                if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any],
                   let choices = json["choices"] as? [[String: Any]],
                   let msg = choices.first?["message"] as? [String: Any],
                   let content = msg["content"] as? String {
                    
                    DispatchQueue.main.async {
                        self.simplifiedText = content
                        self.isAIProcessing = false
                    }
                    
                } else {
                    throw NSError(domain: "", code: -1, userInfo: nil)
                }
                
            } catch {
                DispatchQueue.main.async {
                    self.aiError = "ØªØ¹Ø°Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."
                    self.isAIProcessing = false
                }
                print("JSON Parse Error:", error.localizedDescription)
            }
            
        }.resume()
    }
}
